# Copyright 2016 The Chromium OS Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Google Cloud Storage utilities."""

from __future__ import print_function

import httplib
import httplib2
import logging
import os
import sys

from . import file_utils

try:
  from googleapiclient.discovery import build as discovery_build
  from googleapiclient.http import MediaFileUpload
  from googleapiclient.errors import HttpError
  from oauth2client.service_account import ServiceAccountCredentials
except ImportError:
  # These lines can be used in a requirements.txt file:
  #
  #   google-api-python-client==1.5.1
  #   oauth2client==3.0.0
  #   httplib2==0.9.2
  #   uritemplate==3.0.0
  #
  # Then, to install them:
  #
  #  pip install -t external_dir -r requirements.txt
  _unused_exc_class, _unused_exc, tb = sys.exc_info()
  new_exc = ImportError(
      'Please install these Python libraries before proceeding: '
      'googleapiclient-1.5.1 oauth2client-3.0.0 '
      'httplib2-0.9.2 uritemplate-3.0.0')
  raise new_exc.__class__, new_exc, tb


_GCS_SCOPE = 'https://www.googleapis.com/auth/devstorage.read_write'
_HTTP_TIMEOUT = 60
_MAX_ATTEMPTS = 5
_MIN_CHUNK_SIZE = 256 * 1024  # minimum 256kb according to GCS
_CHUNK_SIZE = 2 * 1024 * 1024  # 2mb
_LOG_CHUNK_SIZE = 10 * 1024 * 1024  # show logging message every 10mb


class CloudStorage(object):
  """Wrapper to access Google Cloud Storage.

  Implements a safe error-retry mechanism around all GCS HTTP requests.
  Provides an interface to necessary GCS methods.

  Properties:
    service: The object provided by googleapiclient on which GCS calls should
             be made.
  """

  def __init__(self, json_key_path, max_attempts=_MAX_ATTEMPTS,
               chunk_size=_CHUNK_SIZE):
    """Makes the connection to GCS.

    Args:
      json_key_path: Path to the private key (in JSON format) on disk.
      max_attempts: The maximum number of times to try any given GCS request
                    before passing the exception on to the caller.
      chunk_size: Files uploaded to GCS are sent in chunks.  Each uploaded chunk
                  will be chunk_size (in bytes), except for the last chunk,
                  which may have size < chunk_size.  Must be at least
                  _MIN_CHUNK_SIZE to adhere to GCS requirements.
    """
    assert chunk_size >= _MIN_CHUNK_SIZE, (
        'Need chunk_size >= %d' % _MIN_CHUNK_SIZE)
    self.max_attempts = max_attempts
    self.chunk_size = chunk_size
    credentials = ServiceAccountCredentials.from_json_keyfile_name(
        json_key_path, scopes=(_GCS_SCOPE,))
    http = credentials.authorize(httplib2.Http(timeout=_HTTP_TIMEOUT))
    self.service = discovery_build('storage', 'v1', http=http)

  def SafeRequest(self, fn):
    """Wraps around requests with a safe error-retry mechanism.

    Ensures that requests don't fail because of non-GCS errors.  Retries
    self.max_attempts times, then passes any exception onto the caller.

    Args:
      fn: Function to be run.  This should be a lambda or a function
          containing a GCS request.
    """
    RETRYABLE_ERRORS = (httplib.HTTPException, httplib2.HttpLib2Error, IOError)
    for attempts in xrange(self.max_attempts):
      try:
        return fn()
      except RETRYABLE_ERRORS:
        logging.debug('Encountered retryable exception', exc_info=True)
        if attempts >= self.max_attempts - 1:
          raise

  def GetObject(self, bucket, name):
    """Get object metadata.

    Args:
      bucket: Requested bucket.
      name: Requested object.

    Returns:
      False if object does not exist.
    """
    try:
      return self.SafeRequest(
          self.service.objects().get(bucket=bucket, object=name).execute)
    except HttpError as e:
      if 404 == e.resp.status:
        return False
      raise

  def InsertObjectRequest(self, source, bucket, name, resumable_uri=None):
    """Uploads an object in chunks, or resumes an existing upload.

    Args:
      source: Path to source file on disk.
      bucket: Target bucket.
      name: Target path within bucket.
      resumable_uri: Set to the URI generated by this function to attempt
                     resuming a transfer.  If resuming does not succeed, the
                     entire file will be transferred.

    Returns:
      A generator.  Every time its next() method is called, another chunk is
      uploaded.  A tuple is generated of the format (resumable_uri, progress,
      response).

      An upload in progress will generate:
        resumable_uri = URI from GCS for this upload
        progress = bytes sent so far
        response = None

      A completed upload will generate:
        resumable_uri = URI from GCS for this upload
        progress = None
        response = response dict from GCS, equivalent to a GetObject call
    """
    media = MediaFileUpload(source, chunksize=self.chunk_size, resumable=True)
    request = self.service.objects().insert(bucket=bucket,
                                            name=name,
                                            media_body=media)

    # If we would like to resume an existing transfer, we can trick the
    # googleapiclient.http.HttpRequest object into thinking that it has
    # encountered an error.  It will query its resumable_uri to find the current
    # position in the file upload.
    response = None
    if resumable_uri:
      request.resumable_uri = resumable_uri
      # Unfortunately we need to use one private variable from the request
      # object, which forces next_chunk() to query GCS for the current position
      # in the file upload, then continuing the upload from there.
      request._in_error_state = True  # pylint: disable=protected-access

      # From https://cloud.google.com/storage/docs/json_api/v1/how-tos/upload:
      # "Handle 404 Not Found errors when doing resumable uploads by starting
      # the entire upload over from the beginning."
      #
      # Try the first chunk to see if we get a 404.
      try:
        progress, response = self.SafeRequest(request.next_chunk)
        progress_bytes = progress.resumable_progress if progress else None
        yield (request.resumable_uri, progress_bytes, response)
      except HttpError as e:
        if 404 == e.resp.status:
          # Start the upload from the beginning by omitting the resumable_uri
          # argument.  Annoying trick to pass all the generator elements along.
          for ret in self.InsertObjectRequest(source, bucket, name):
            yield ret
          return

    while response is None:
      progress, response = self.SafeRequest(request.next_chunk)
      progress_bytes = progress.resumable_progress if progress else None
      yield request.resumable_uri, progress_bytes, response

  def GetBucket(self, bucket):
    """Gets bucket metadata.

    Args:
      bucket: Requested bucket.

    Returns:
      False if bucket does not exist.
    """
    try:
      return self.SafeRequest(self.service.buckets().get(bucket=bucket).execute)
    except HttpError as e:
      if 404 == e.resp.status:
        return False

  def UploadFile(self, local_path, target_path, resumable_uri=None):
    """Attempts to upload a file to GCS, with resumability.

    Args:
      local_path: Path to the file on local disk.
      target_path: Target path, including bucket and initial '/'.
      resumable_uri: Resumable URI to use from previous failure.

    Returns:
      None on success, resumable_uri on failure.  This can be fed back in
      to a subsequent call as an argument in order to resume the upload.

    Raises:
      OSError if file does not exist.
    """
    local_size = os.path.getsize(local_path)
    local_md5 = file_utils.MD5InBase64(local_path)
    # Weird behaviour when we have an empty target_path.
    assert target_path, 'Must specify target_path'

    # Check to see whether target already exists.
    bucket, _unused_slash, target_file = target_path[1:].partition('/')
    logging.info('Checking for existence of %s...', target_path)
    response = self.GetObject(bucket=bucket, name=target_file)
    if response:
      remote_size = int(response['size'])
      remote_md5 = response['md5Hash']
      if remote_size == local_size and remote_md5 == local_md5:
        logging.warning(
            'File already exists on remote end with same size (%d) '
            'and same MD5 hash (%s); skipping',
            remote_size, remote_md5)
        return
      else:
        logging.error(
            'File already exists on remote end, but size or MD5 hash doesn\'t '
            'match; size on remote %s = %d, size on local %s = %d; '
            'will overwrite',
            target_path, remote_size, local_path, local_size)

    # Upload the data in chunks.  If we get any unexpected exceptions, assume
    # failure and let the user retry.
    log_threshold = _LOG_CHUNK_SIZE
    try:
      request = self.InsertObjectRequest(source=local_path, bucket=bucket,
                                         name=target_file,
                                         resumable_uri=resumable_uri)
      response = None
      while response is None:
        resumable_uri, progress, response = request.next()
        if progress and progress >= log_threshold:
          # Log upload status after threshold is reached.
          logging.info('GCS upload %.1f%% of %s...' % (
                       float(progress) / local_size * 100, target_path))
          log_threshold = progress + _LOG_CHUNK_SIZE
    except Exception:
      logging.exception('Exception encountered on Cloud Storage upload')
      return resumable_uri

    # Check that the transfer succeeded.
    confirmed_size = int(response['size'])
    confirmed_md5 = response['md5Hash']
    if confirmed_size != local_size or confirmed_md5 != local_md5:
      logging.error('Size or MD5 mismatch after a put action; local_size = %d, '
                    'confirmed_size = %d; local_md5 = %s, confirmed_md5 = %s',
                    local_size, confirmed_size, local_md5, confirmed_md5)
      return resumable_uri
