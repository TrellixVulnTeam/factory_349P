#!/usr/bin/python
# Copyright 2015 The Chromium OS Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Google Cloud Storage implementation for FetchSource and UploadTarget."""

import base64
import hashlib
import httplib
import httplib2
import logging
import os
import sys
import traceback
import yaml

from common import (AtomicWrite, ComputePercentage, GetMetadataPath,
                    GetOrCreateMetadata, RegenerateUploaderMetadataFile,
                    UPLOADER_METADATA_DIRECTORY)
from uploader_exception import UploaderConnectionError, UploaderFieldError
from uploader_interface import FetchSourceInterface, UploadTargetInterface

try:
  from googleapiclient.discovery import build as discovery_build
  from googleapiclient.http import MediaFileUpload
  from googleapiclient.errors import HttpError
  from oauth2client.client import SignedJwtAssertionCredentials
except ImportError:
  raise ImportError('Please install these Python libraries before proceeding: '
                    'googleapiclient oauth2client httplib2 uritemplate')


_MD5_READ_BLOCK_SIZE = 65536


def _MD5Sum(path):
  """Calculate MD5 hash of a file on the local machine.

  Although a similar function already exists in utils/file_utils.py, we
  reproduce it here to prevent a dependency on factory code.  Uploader is meant
  to be run portably.

  From: http://stackoverflow.com/questions/1742866/compute-crc-of-file-in-python

  Args:
    path: local path of the file.

  Returns:
    MD5 hash of the given file.
  """
  md5 = hashlib.md5()
  with open(path, 'rb') as f:
    for chunk in iter(lambda: f.read(_MD5_READ_BLOCK_SIZE), ''):
      md5.update(chunk)
  return md5.digest()


class CloudStorage(object):
  """Wrapper to access Google Cloud Storage.

  Implements a safe error-retry mechanism around all GCS HTTP requests.
  Provides an interface to necessary GCS methods.

  Properties:
    MAX_ATTEMPTS: the maximum number of times to try any given GCS request
                  before passing the exception on to the caller.
    CHUNK_SIZE: files uploaded to GCS are sent in chunks.  the size of this
                chunk in bytes (except for the last chunk, which may be
                size < CHUNK_SIZE).
    service: the object provided by googleapiclient on which GCS calls should be
             made.
  """
  MAX_ATTEMPTS = 5
  CHUNK_SIZE = 2 * 1024 * 1024  # minimum 1024 * 256 according to GCS
  service = None

  def __init__(self, client_email, private_key):
    """Makes the connection to GCS."""
    credentials = SignedJwtAssertionCredentials(
        client_email, private_key,
        'https://www.googleapis.com/auth/devstorage.read_write')
    http = credentials.authorize(httplib2.Http())
    self.service = discovery_build('storage', 'v1', http=http)

  def SafeRequest(self, fn, max_attempts=MAX_ATTEMPTS):
    """Wraps around requests with a safe error-retry mechanism.

    Ensures that requests don't fail because of non-GCS errors.  Retries
    max_attempts times, then passes any exception onto the caller.

    Args:
      fn: function to be run.  usually this is a lambda around a GCS request.
      max_attempts: number of times to try before failing and passing the
                    exception onto the caller.
    """
    RETRYABLE_ERRORS = (httplib.HTTPException, httplib2.HttpLib2Error, IOError)
    attempts = max_attempts
    for attempts in xrange(max_attempts):
      try:
        return fn()
      except RETRYABLE_ERRORS:
        if attempts >= max_attempts - 1:
          raise

  def GetObject(self, bucket, name):
    """Get object metadata.

    Args:
      bucket: requested bucket.
      name: requested object.

    Returns:
      False if object does not exist.
    """
    try:
      return self.SafeRequest(
          lambda: self.service.objects().get(bucket=bucket,
                                             object=name).execute())
    except HttpError as e:
      if 404 == e.resp.status:
        return False

  def InsertObject(self, source, bucket, name, resumable_uri=None):
    """Upload an object in chunks, or resume an existing upload.

    Args:
      source: path to source file on disk.
      bucket: target bucket.
      name: target path within bucket.
      resumable_uri: set to the URI generated by this function to attempt
                     resuming a transfer.  if resuming does not succeed, the
                     entire file will be transferred.

    Returns:
      A generator.  Every time its next() method is called, another chunk is
      uploaded.  A tuple is generated of the format (resumable_uri, progress,
      response).

      An upload in progress will generate:
        resumable_uri = URI from GCS for this upload
        progress = googleapiclient.http.MediaUploadProgress object
        response = None

      A completed upload will generate:
        resumable_uri = URI from GCS for this upload
        progress = None
        response = response dict from GCS, equivalent to a GetObject call
    """
    media = MediaFileUpload(source, chunksize=self.CHUNK_SIZE, resumable=True)
    request = self.service.objects().insert(bucket=bucket,
                                            name=name,
                                            media_body=media)

    # If we would like to resume an existing transfer, we can trick the
    # googleapiclient.http.HttpRequest object into thinking that it has
    # encountered an error.  It will query its resumable_uri to find the current
    # position in the file upload.
    response = None
    if resumable_uri:
      request.resumable_uri = resumable_uri
      # Unfortunately we need to use one private variable from the request
      # object, which forces next_chunk() to query GCS for the current position
      # in the file upload, then continuing the upload from there.
      request._in_error_state = True  # pylint: disable=W0212

      # From https://cloud.google.com/storage/docs/json_api/v1/how-tos/upload:
      # "Handle 404 Not Found errors when doing resumable uploads by starting
      # the entire upload over from the beginning."
      #
      # Try the first chunk to see if we get a 404.
      try:
        progress, response = self.SafeRequest(request.next_chunk)
        yield (request.resumable_uri, progress, response)
      except HttpError as e:
        if 404 == e.resp.status:
          # Start the upload from the beginning by omitting the resumable_uri
          # argument.  Annoying trick to pass all the generator elements along.
          g = self.InsertObject(source, bucket, name)
          while True:
            try:
              yield g.next()
            except StopIteration:
              break
          return

    while response is None:
      progress, response = self.SafeRequest(request.next_chunk)
      yield (request.resumable_uri, progress, response)

  def GetBucket(self, bucket):
    """Get bucket metadata.

    Args:
      bucket: requested bucket.

    Returns:
      False if bucket does not exist.
    """
    try:
      return self.SafeRequest(
          lambda: self.service.buckets().get(bucket=bucket).execute())
    except HttpError as e:
      if 404 == e.resp.status:
        return False


class GCSBase(object):
  """Common function shared across FetchSource and UploadTarget on GCS.

  The GCSBase class is inherited by FetchSource and UploadTarget, designed
  for GCS protocol and key exchange authentication.

  Properties:
    config_name: name for the GCS connection.
    client_email: the client email shown on the Credentials page in Google
                  Developers Console.
    private_key_path: the path to the private key (only p12 format is currently
                      supported) generated and downloaded from the Credentials
                      page in Google Developers Console.
    bucket: the bucket which will be accessed
    archive_path: the prefix of files within the bucket to be accessed, without
                  a trailing slash.  thus, the full GCS path of <file> would be:
                  /<bucket>/<archive_path>/<file>
    gcs: a connected instance of CloudStorage.
  """
  config_name = None
  client_email = None
  private_key_path = None
  bucket = None
  archive_path = None
  gcs = None

  def LoadConfiguration(self, config, config_name=None):
    # Ingest the config dict.
    # archive_path should be in one of these formats, and will be split into its
    # [bucket] and [path] portions:
    #   /[bucket]
    #   /[bucket]/
    #   /[bucket]/[path]
    #   /[bucket]/[path]/

    self.config_name = config_name
    self.client_email = config['client_email']
    self.private_key_path = config['private_key']
    archive_path_parts = config['archive_path'].rstrip('/').split('/')
    self.bucket = archive_path_parts[1]
    self.archive_path = '/'.join(archive_path_parts[2:])

    # Try to connect and check the existence of archive_path.
    self.gcs = self._Connect()

    config_type = self.__class__.__name__  # 'FetchSource' or 'UploadTarget'
    if not self.CheckDirectory(self.archive_path):
      error_msg = ('%s directory %r doesn\'t exist'
                   % (config_type, self.archive_path))
      raise UploaderFieldError(error_msg)

    logging.info('%s configuration %r loaded.',
                 config_type, config_name)

  def _Connect(self):
    """Tries to authenticate to Google API server.

    Returns:
      A connected instance of CloudStorage.

    Raises:
      UploaderConnectionError if all retries failed.
    """
    try:
      with open(self.private_key_path) as f:
        private_key = f.read()
      return CloudStorage(self.client_email, private_key)
    except Exception:
      # The exception raised has useful information that we should pass on
      # through our own UploaderConnectionError.
      exc_type, exc_value, _ = sys.exc_info()
      raise UploaderConnectionError(
          'Cannot authenticate to Google API server. %s'
          % traceback.format_exception_only(exc_type, exc_value)[-1])

  def CheckDirectory(self, _):
    """Checks if the directory exists.

    Since GCS doesn't have the concept of 'directories', we simply check
    that the bucket exists instead.
    """
    return self.gcs.GetBucket(bucket=self.bucket)

  def CalculateDigest(self, relative_path):
    """CalculateDigest is not used in our implementation of UploadTarget."""
    raise NotImplementedError

  def MoveFile(self, from_path, to_path):
    """MoveFile is not used in our implementation of UploadTarget."""
    raise NotImplementedError

  def CreateDirectory(self, dir_path):
    """CreateDirectory is not used in our implementation of UploadTarget."""
    raise NotImplementedError


class FetchSource(GCSBase):  # pylint: disable=W0223
  """A GCS implementation of FetchSourceInterface.

  Since we never encounter a scenario where we need to use uploader with GCS as
  a source, simply raise NotImplementedError.  These functions will never be
  called.
  """
  __implements__ = (FetchSourceInterface, )  # For pylint

  def ListFiles(self):
    raise NotImplementedError

  def FetchFile(self, source_path, target_path,
                metadata_path=None, resume=True):
    raise NotImplementedError


class UploadTarget(GCSBase):  # pylint: disable=W0223
  """A GCS implementation of UploadTargetInterface."""
  __implements__ = (UploadTargetInterface, )  # For pylint

  def UploadFile(self, local_path, target_path,
                 metadata_path=None, resume=True):
    def _UpdateUploadMetadata(uploaded_bytes, resumable_uri):
      metadata = GetOrCreateMetadata(
          metadata_path, RegenerateUploaderMetadataFile)
      logging.info(
          'Uploading...%9.5f%% of %10d bytes',
          ComputePercentage(uploaded_bytes, local_size), local_size)
      upload_metadata = metadata.setdefault('upload', {})
      upload_metadata.update(
          {'protocol': 'GCS',
           'bucket': self.bucket,
           'archive_path': self.archive_path,
           'path': target_path,
           'resumable_uri': resumable_uri,
           'uploaded_bytes': uploaded_bytes,
           'percentage': ComputePercentage(uploaded_bytes, local_size)})
      AtomicWrite(metadata_path, yaml.dump(metadata, default_flow_style=False))

    # Check if file to upload exists.
    if not os.path.isfile(local_path):
      raise UploaderFieldError(
          '%r doesn\'t exist on local file system', local_path)

    if metadata_path is None:
      metadata_path = GetMetadataPath(
          local_path, UPLOADER_METADATA_DIRECTORY)

    # Convert target_path into full_path on the GCS side.
    target_path = '/'.join([self.archive_path, target_path])
    local_size = os.path.getsize(local_path)
    local_md5 = _MD5Sum(local_path)

    # Check to see whether target already exists.
    logging.info('Checking for existence of /%s/%s...',
                 self.bucket, target_path)
    response = self.gcs.GetObject(bucket=self.bucket, name=target_path)
    if response:
      remote_size = int(response['size'])
      remote_md5 = base64.b64decode(response['md5Hash'])
      if remote_size == local_size and remote_md5 == local_md5:
        logging.error(
            'File already exists on remote end with same size (%d) '
            'and same MD5 hash (base64 encoded MD5 from GCS: %s). Skipping.',
            remote_size, response['md5Hash'])
        _UpdateUploadMetadata(uploaded_bytes=remote_size,
                              resumable_uri=None)
        return True
      else:
        logging.error(
            'File already exists on remote end, but size or MD5 hash doesn\'t '
            'match. Size on remote %r = %10d, size on local %r = %10d. '
            'Will overwrite it.',
            target_path, remote_size, local_path, local_size)

    # Check to see if this upload is already in progress.
    resumable_uri = None
    if resume:
      metadata = GetOrCreateMetadata(
          metadata_path, RegenerateUploaderMetadataFile)
      try:
        resumable_uri = metadata['upload']['resumable_uri']
      except Exception:
        logging.error(
            'A resume was requested, but resumable_uri could not be '
            'retrieved from local metadata. Will restart upload.')

    # Upload the data in chunks.  If we get any unexpected exceptions, assume
    # failure and let uploader retry.
    try:
      request = self.gcs.InsertObject(source=local_path, bucket=self.bucket,
                                      name=target_path,
                                      resumable_uri=resumable_uri)
      response = None
      while response is None:
        resumable_uri, progress, response = request.next()
        if progress:
          _UpdateUploadMetadata(uploaded_bytes=progress.resumable_progress,
                                resumable_uri=resumable_uri)
    except Exception as e:
      logging.error(
          'Exception encountered on upload. Will reupload. %s: %s',
          e.__class__.__name__, e.message)
      return False

    # Double-check that the transfer succeeded.
    confirmed_size = int(response['size'])
    confirmed_md5 = base64.b64decode(response['md5Hash'])
    if confirmed_size != local_size or confirmed_md5 != local_md5:
      logging.error('Size or MD5 mismatch after a put action. local_size = %d, '
                    'confirmed_size = %d. base64 encoded MD5 from GCS: %s. '
                    'Will reupload.',
                    local_size, confirmed_size, response['md5Hash'])
      confirmed_size = 0

    # Update metadata one last time to remove the resumable URI.
    _UpdateUploadMetadata(uploaded_bytes=confirmed_size,
                          resumable_uri=None)

    return confirmed_size == local_size
